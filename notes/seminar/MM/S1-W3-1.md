### [CVPR 2025] RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models [üóÇÔ∏è](https://arxiv.org/abs/2410.13360)


- Motivation
  - Limitation of Existing MLLM personalization
    - Strong performance in general capabilities, such as image captioning and visual question answering
    - However, lack of user-specific knowledge still restricts their application in human's daily life
    - Collecting personal data and training a separate model for each user is impractical, leading to issues of  cost and privacy
  - Advances and Limitations in MLLM personalization
    - MyVLM and Yo'LLaVA require training external classifiers or learning special tokens to recognize specific concepts
    - They require a large number of labeled images and negative samples, and retraining is needed whenever new concepts emerge
- Preliminary
  - Personalization for MLLMs
    - Example: <K> lives in Korea. <J> is <K>'s boyfriend.
      - (1) Personalized Captioning
        - ü§ñ: <K> and <J> enjoying a relaxing afternoon at a trendy cafe.
      - (2) Personalized Conversation
        - üëß: What is <K> doing?
        - ü§ñ: <K> is sitting at a table in a cafe, wearing a blue polka-dot dress. She is holding a glass of a pinkish drink with a straw and appears to be sipping from it.
- Method
  - Remember
    - Remember personal concepts and relevant information
    - Databese $\mathbf{M}$ stores visual feature $k_j$, image $I_j$, brief description $T_j$
    - 
  - Retrieve
