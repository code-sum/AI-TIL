### [CVPR 2025] RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models [üóÇÔ∏è](https://arxiv.org/abs/2410.13360)


- Motivation
  - Limitation of Existing MLLM personalization
    - Strong performance in general capabilities, such as image captioning and visual question answering
    - However, lack of user-specific knowledge still restricts their application in human's daily life
    - Collecting personal data and training a separate model for each user is impractical, leading to issues of  cost and privacy
  - Advances and Limitations in MLLM personalization
    - MyVLM and Yo'LLaVA require training external classifiers or learning special tokens to recognize specific concepts
    - They require a large number of labeled images and negative samples, and retraining is needed whenever new concepts emerge
- Preliminary
  - Personalization for MLLMs
    - Example: <K> lives in Korea. <J> is <K>'s boyfriend.
      - (1) Personalized Captioning
        - ü§ñ: <K> and <J> enjoying a relaxing afternoon at a trendy cafe.
      - (2) Personalized Conversation
        - üëß: What is <K> doing?
        - ü§ñ: <K> is sitting at a table in a cafe, wearing a blue polka-dot dress. She is holding a glass of a pinkish drink with a straw and appears to be sipping from it.
- Method
  - Remember
    - Remember personal concepts and relevant information
    - Databese $\mathbf{M}$ stores visual feature $k_j$, image $I_j$, brief description $T_j$
    - User input $Q = (X_v, X_q)$ and crop region $X_u = R(X_v, X_q|P)$ ($P$: predefined setting, $R$: detection model)
  - Retrieve
    - Each recognized component $v_i = \mathcal{E}(\mathbf{X}_u^i)$ ($\mathcal{E}$: image encoder)
    - $Dist(v_i, k_j) = \|v_i - k_j\|$, Top K image-text pairs $\{(\mathbf{I}_1, \mathbf{T}_1), (\mathbf{I}_2, \mathbf{T}_2), \cdots (\mathbf{I}_K, \mathbf{T}_K)\}$
- Experiment
  - Table 3. Quantitative Evaluation on Image Captioning. The best result in each metric is bold.
    | Method | LLM | Recall | Precision | F1-score |
    | :--- | :--- | :--- | :--- | :--- |
    | LLaVA [28] + Retriever | Vicuna-13B | 1.260 | 48.76 | 2.450 |
    | LLaVA-LoRA [17] | Vicuna-13B | 82.97 | 93.28 | 87.82 |
    | MyVLM-LLaVA [2] | Vicuna-13B | 84.65 | 86.37 | 85.50 |
    | RAP-LLaVA | Vicuna-13B | **93.51** | **96.47** | **94.97** |
    | RAP-Phi3-V | Phi3-V-3.8B | 88.14 | 95.10 | 91.49 |
- Conclusion
  - Novelty
    - Retrieval-Augmented Personalization
      - Achieves personalization with just a single image and a short description, while supporting real-time updates without retraining.
    - Robustness through Negative sampling
      - improves discrimination of real concepts from noise, enhancing robustness.
  - Weakness
    - Dependency on Retrieval Quality
      - As shown in the skip retrieval experiment, the overall performance is sensitive to the quality of the retrieval stage.
    - Complex Dataset Construction
      - Relies on diverse components which could introduce significant cost and complexity.
