### [CVPR 2024] InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks [ðŸ—‚ï¸](https://arxiv.org/abs/2312.14238)


- Related Works
  - Vision Large Language Models: lightweight "glue" layers

- Limitations
  - Parameter Scales
    - Modality Encoder: ~ 1 billion
    - LLM: 1000 billion

- Architecture
  - Key Ideas
    - Parameter-balanced vision and language components
      - InternViT-6B(Vision Encoder) > [cross attention] > QLLaMA-8B(Language Middleware) > [MLP] > Vicuna-13B(LLM)
  - Model Design: QLLaMA
    - To align visual and linguistic features
    - Pre-trained LLaMA + 96 learnable queries + cross-attention layers
    - 42 times larger than QFormer
    - Can be applied to contrastive learning

- The training strategy of the proposed InternVL model
  - stage 1: contrastive pre-training
    - supported tasks
      - (1) zero-shot image classification (new)
      - (2) zero-shot image-text retrieval (new)
  - stage 2: generative pre-training
    - supported tasks
      - (1) zero-shot image classification
      - (2) zero-shot image-text retrieval
      - (3) zero-shot image captioning (new)
  - stage 3: supervised fine-tuning
    - supported tasks
      - (4) multi-modal dialogue (new)
      - (5) visual question answering (new)

- Alignment Strategy
  - Contrastive Learning
    - (1) To align InternViT-6B with LLaMA-7B
    - (2) Publicly available, multilingual datasets
    - (3) CLIP object function

- Experiments
  - Visual Perception Benchmarks
    - Image Classification (Table 4)
    - Semantic Segmentation (Table 5)
