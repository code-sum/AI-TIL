### [CVPR 2024] InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks [ðŸ—‚ï¸](https://arxiv.org/abs/2312.14238)


- Related Works
  - Vision Large Language Models: lightweight "glue" layers

- Limitations
  - Parameter Scales
    - Modality Encoder: ~ 1 billion
    - LLM: 1000 billion

- Architecture
  - Key Ideas
    - Parameter-balanced vision and language components
      - InternViT-6B(Vision Encoder) > [cross attention] > QLLaMA-8B(Language Middleware) > [MLP] > Vicuna-13B(LLM)
  - Model Design: QLLaMA
    - To align visual and linguistic features
