### [SIGCOMM 2024] NetLLM: Adapting Large Language Models for Networking [🗂️](https://arxiv.org/abs/2402.02338)


- 3줄 요약
  - 기존 딥러닝 기반 네트워킹 알고리즘은 높은 모델 엔지니어링 비용과 낮은 일반화 성능 문제를 겪고 있으며, 본 연구는 이러한 한계를 극복하기 위해 LLM 을 네트워킹 분야의 파운데이션 모델로 활용하는 NetLLM 프레임워크를 제안
  - NetLLM은 LLM이 다양한 네트워킹 데이터를 효과적으로 처리하도록 멀티모달 인코더를, 태스크별로 신뢰할 수 있고 효율적인 답변을 생성하도록 네트워킹 헤드를, 그리고 도메인 지식 습득 비용을 크게 줄이는 데이터 기반 저랭크 네트워킹 적응(DD-LRNA) 방식을 포함하는 통합 설계를 제공
  - NetLLM으로 적응된 LLM은 뷰포트 예측, 적응형 비트레이트 스트리밍, 클러스터 작업 스케줄링 등 세 가지 네트워킹 사용 사례 전반에 걸쳐 최첨단 알고리즘을 능가하는 성능과 뛰어난 일반화 능력을 보임
 
- Motivation
  - VP(viewport prediction): VR 에서 사용자 시선이 어딜 볼 것인지 미리 예측하고 고화질로 보여져야 하는 곳, 저화질로 보여져도 되는 곳에 각각 다른 화질의 영상을 전송하는 기술
  - ABR(adaptive bitrate streaming): 네트워크 상태에 따라 동영상 화질을 실시간으로 조정하는 기술
  - CJS(cluster job scheduling): 여러 대의 컴퓨터에게 어떤 일을, 어떤 컴퓨터가, 언제 처리할지 미리 정해주는 관리자
  - 각 Task 마다 DNN 모델 수동 설계 > 엔지니어링 오버헤드, 일반화 성능 저조 > LLM 의 가능성에 집중

- NetLLM 설계: 3가지 모듈로 구성
  - (1) 멀티모달 인코더: 네트워킹 입력을 토큰 임베딩으로 변환
    - 이미지: ViT, 시계열: 1D-CNN, 스칼라: FC, 그래프: GNN 등
    - 이미지에는 Vision Transformer (ViT), 시계열 및 시퀀스 데이터에는 1D Convolutional Neural Network (1D-CNN), 스칼라 데이터에는 Fully Connected Layer, 그래프 정보에는 Graph Neural Network (GNN)와 같은 modality-specific feature encoder를 재사용하여 특징(feature) 추출
    - 추출된 특징들은 trainable linear layer와 Layer Normalization을 통해 LLM 의 token space 에 맞는 embedding 벡터로 투영됨
    - 예를 들어, VP task 의 경우 ViT와 1D-CNN이 이미지 및 시계열 뷰포트 데이터를 인코딩한 후, 별도의 linear projection layer를 통해 token-like embedding 으로 변환됨
  - (2) 네트워킹 헤드: 기존 LLM의 LM헤드 대신, 과제별 변환기로서 직접적으로 과제에 맞는 답변(변수 범위 내) 생성
    - 네트워킹 헤드는 토큰 예측 방식을 없애고, 유효한 답변 범위(예: ABR에서 비트레이트 옵션 선택) 내에서 직접 답변을 생성하여 LLM의 신뢰성을 보장하고 답변 생성 latency 줄임
    - 예를 들어, ABR task 의 경우 Networking head는 비트레이트의 확률 분포를 직접 예측하여 단일 추론으로 유효한 비트레이트를 선택
    - VP 태스크의 경우 roll, pitch, yaw 값을 출력하는 세 개의 뉴런으로 구성
  - (3) DD-LRNA(Data-driven Low-Rank Networking Adaptation): 데이터 기반 적응 파이프라인 및 저차원 행렬로 파라미터 미세 조정(적은 학습 비용/메모리 소모)
    - LLM의 fine-tuning 비용을 획기적으로 줄이기 위한 scheme. 다음의 두 가지 핵심 디자인으로 구성
        - ① Data-driven adaptation pipeline
        - ② Low-rank adaptation
- LLM은 파라미터 동결 상태에서 위 인코더/헤드/저차원 행렬만 미세 조정

- NetLLM 구현 및 실험결과
  - Llama2-7B 기반 LLM 사용하여 VP, ABR, CJS 세 가지 네트워킹 태스크에 대한 광범위한 평가를 수행
  - NetLLM 은 trace-driven 시뮬레이션 및 실제 환경 테스트에서 VP 의 경우 10.1-36.6%, ABR 의 경우 14.5-36.6%, CJS 의 경우 6.8-41.3% 의 성능 향상을 이뤄 최신 알고리즘을 능가하는 뛰어난 성능과 더 강력한 일반화 능력을 보여줌
  - 또한, LLM 의 사전 학습된 지식과 학습된 domain knowledge 가 네트워킹 adaptation 에 필수적임을 입증했으며, LLM 의 종류와 크기가 성능에 미치는 영향도 분석
  - NetLLM-adapted LLM 은 7B 파라미터 크기에서 29GB 메모리를 사용하고 0.1~0.3초 내에 답변을 생성하며, OPT-1.3B 와 같은 작은 LLM 은 7GB 메모리와 0.04초의 답변 생성 시간으로도 우수한 성능을 보여, 실제 환경 배포 가능성을 시사
  - 
