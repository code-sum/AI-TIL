# âœ… Ragas

> **[ì°¸ê³ ìë£Œ]**
>
> - Ragas ê³µì‹ë¬¸ì„œ [(link)](https://docs.ragas.io/en/latest/)
> - Ragas Github [(link)](https://github.com/explodinggradients/ragas)
> - LangChain "RAG Evaluation" Webinar [(link)](https://www.youtube.com/watch?v=fWC4VxolWAk) (23.08.25)
> - [LangChain Blog] [Evaluating RAG pipelines with Ragas + LangSmith](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/) (23.08.23)
> - [Medium] [RAG Evaluation](https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4) (23.09.01)
> - [Medium] [Combining Ragas (RAG Assessment Tool) with LangSmith](https://cobusgreyling.medium.com/combining-ragas-rag-assessment-tool-with-langsmith-e46078001f95) (23.09.06)



- Ragas
  - *"Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines."*
  - í‰ê°€, ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ í”„ë¡œë•ì…˜ ë‹¨ê³„ì˜ LLM & RAG application ì„±ëŠ¥ì„ ë†’ì„



- Ragas ì˜ í‰ê°€ ëŒ€ìƒ

  > ì•„ë˜ 2ê°€ì§€ ì»´í¬ë„ŒíŠ¸ëŠ” í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‚˜ëˆ„ëŠ” ì¹´í…Œê³ ë¦¬ê¸°ë„ í•˜ë©°, QA pipeline ì„ í‰ê°€í•˜ê¸° ìœ„í•´ì„  2ê°€ì§€ ì»´í¬ë„ŒíŠ¸ ëª¨ë‘ë¥¼ í‰ê°€ ëŒ€ìƒìœ¼ë¡œ ì‚¼ì•„ì•¼ í•¨

  1. ê²€ìƒ‰ê¸°(Retriever) : ì…ë ¥ëœ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ìˆì–´ ê°€ì¥ ì—°ê´€ì„± ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰
  2. ìƒì„±ê¸°(Generator) : ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±



- Ragas Score

  > ì•„ë˜ 4ê°€ì§€ í•­ëª©ì˜ ì¡°í™” í‰ê· (harmonic mean) ê³„ì‚°

  ![1223_Ragas](1223_Ragas.assets/1223_Ragas.png)

  - ìƒì„±(generation) ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ
    - `faithfulness` : ë‹µë³€ì˜ ì •í™•ë„. ìƒì„±ëœ ë‹µë³€ì— í™˜ê°í˜„ìƒ(hallucinations)ì´ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€(=í˜¹ì€ ìƒì„±ëœ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€) ì¸¡ì •
    - `answer_relevancy` : ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ë„. ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ ë‚´ìš©ê³¼ ì–¼ë§ˆë‚˜ ì—°ê´€ì„±ì´ ë†’ì€ì§€ ì¸¡ì •
  - ê²€ìƒ‰(retrieval) ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ
    ğŸ’¡ [Machine Learning] precision vs. recall ê°œë… êµ¬ë¶„ [(link)](https://sumniya.tistory.com/26)
    - `context_precision` : context ë‚´ë¶€ chunks ë“¤ì´ ground_truth(=ì‚¬ì‹¤ì´ë¼ê³  ì •ì˜ëœ ê²ƒë“¤)ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€ë¥¼ ê³„ì‚°
      - (ML ì—ì„œ precision ì€ ëª¨ë¸ì´ Trueë¼ê³  ë¶„ë¥˜í•œ ê²ƒ ì¤‘ì—ì„œ ì‹¤ì œ Trueì¸ ê²ƒì˜ ë¹„ìœ¨)
    - `context_recall` : ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê³¼ì •ì—ì„œ â€˜í•„ìš”í•œ ëª¨ë“  í•„ìˆ˜ ì •ë³´â€™ë¥¼ â€˜ê²€ìƒ‰ê¸°â€™ê°€ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê³„ì‚°
      - (ML ì—ì„œ recall ì€ ì‹¤ì œ Trueì¸ ê²ƒ ì¤‘ì—ì„œ ëª¨ë¸ì´Â Trueë¼ê³  ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨)



- Ragas í™œìš© ì˜ˆì‹œ

  1. `pip install ragas`

  2. í‰ê°€ë¥¼ ìœ„í•œ Chain ë§Œë“¤ê¸°

  3. `from ragas.metrics` ì—ì„œ í‰ê°€ì— í•„ìš”í•œ ëª¨ë“  í•­ëª© import 

  4. `from ragas.langchain` ì—ì„œ `RagasEvaluatorChain` import

     - ragas í‰ê°€ í•­ëª©ì„ LangChain [EvaluationChain](https://python.langchain.com/docs/guides/evaluation/?ref=blog.langchain.dev) ìœ¼ë¡œ ë³€í™˜ì‹œì¼œ ì£¼ëŠ” ì²´ì¸

     ```python
     from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
     from ragas.langchain import RagasEvaluatorChain
     
     # make eval chains
     eval_chains = {
         m.name: RagasEvaluatorChain(metric=m) 
         for m in [faithfulness, answer_relevancy, context_precision, context_recall]
     }
     ```

  5. ìœ„ ì½”ë“œë¡œ í‰ê°€ìš© ì²´ì¸ì´ ë§Œë“¤ì–´ì§€ë©´, ì•„ë˜ ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ í‰ê°€ìš© ì²´ì¸ì„ í˜¸ì¶œ

     ```python
     # evaluate
     for name, eval_chain in eval_chains.items():
         score_name = f"{name}_score"
         print(f"{score_name}: {eval_chain(result)[score_name]}")
     
     # í‰ê°€ìš© ì²´ì¸ í˜¸ì¶œì€ __call__() ë©”ì„œë“œ í™œìš©
     
     # output
     # faithfulness_score: 1.0
     # answer_relevancy_score: 0.9193459596511587
     # context_precision_score: 0.07480974380786602
     # context_recall_score: 0.9193459596511587
     ```

     

